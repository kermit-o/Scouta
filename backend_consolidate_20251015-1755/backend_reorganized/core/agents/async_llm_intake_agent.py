"""
Async LLM Intake Agent - Consulta REAL y AS√çNCRONA a DeepSeek
"""
import json
import asyncio
from services.robust_deepseek_client import RobustDeepSeekClient

class AsyncLLMIntakeAgent:
    def __init__(self):
        self.client = RobustDeepSeekClient()
    
    async def run(self, project_id: str, user_requirements: dict) -> dict:
        """Consulta AS√çNCRONA al LLM para an√°lisis de requisitos"""
        print("üß† Async LLM Intake Agent - Consultando DeepSeek...")
        
        try:
            # Construir prompt para el LLM
            prompt = f"""
            Como arquitecto de software IA, analiza ESTOS requisitos y genera especificaciones t√©cnicas detalladas:

            REQUISITOS DEL USUARIO: {json.dumps(user_requirements, indent=2)}

            GENERA un an√°lisis t√©cnico estructurado que incluya:
            1. Componentes espec√≠ficos necesarios
            2. Stack tecnol√≥gico recomendado  
            3. Arquitectura del sistema
            4. Caracter√≠sticas t√©cnicas detalladas
            5. Dependencias y integraciones

            Responde en formato JSON v√°lido.
            """
            
            # Consulta AS√çNCRONA REAL al LLM
            analysis = await self.client.generate_code(prompt, "An√°lisis de Requisitos T√©cnicos")
            
            # Parsear respuesta del LLM
            structured_analysis = self._parse_llm_response(analysis, user_requirements)
            
            print(f"‚úÖ An√°lisis LLM completado - {len(structured_analysis.get('components', []))} componentes")
            return structured_analysis
            
        except Exception as e:
            print(f"‚ùå Error en an√°lisis LLM: {e}")
            return self._fallback_analysis(user_requirements)
    
    def _parse_llm_response(self, llm_response: str, original_requirements: dict) -> dict:
        """Parsea la respuesta del LLM a estructura consistente"""
        try:
            # Intentar extraer JSON de la respuesta
            if '{' in llm_response and '}' in llm_response:
                json_start = llm_response.find('{')
                json_end = llm_response.rfind('}') + 1
                json_str = llm_response[json_start:json_end]
                parsed = json.loads(json_str)
                
                return {
                    "project_name": original_requirements.get("name", "Proyecto Generado"),
                    "project_type": parsed.get("project_type", "web_app"),
                    "components": parsed.get("components", []),
                    "tech_stack": parsed.get("tech_stack", []),
                    "architecture": parsed.get("architecture", {}),
                    "features": parsed.get("features", []),
                    "llm_analysis": llm_response[:500] + "..." if len(llm_response) > 500 else llm_response,
                    "status": "analyzed_by_llm"
                }
        except Exception as e:
            print(f"‚ö†Ô∏è No se pudo parsear respuesta LLM: {e}")
        
        # Fallback si no se puede parsear
        return self._fallback_analysis(original_requirements)
    
    def _fallback_analysis(self, requirements: dict) -> dict:
        """An√°lisis de fallback cuando el LLM falla"""
        return {
            "project_name": requirements.get("name", "Proyecto"),
            "project_type": requirements.get("type", "web_app"),
            "components": [
                {"name": "Frontend Principal", "type": "frontend", "description": "Interfaz de usuario principal"},
                {"name": "API Backend", "type": "backend", "description": "Servicios y l√≥gica de negocio"},
                {"name": "Base de Datos", "type": "database", "description": "Almacenamiento persistente"}
            ],
            "tech_stack": ["React", "Node.js", "MongoDB"],
            "features": requirements.get("features", ["feature1"]),
            "status": "fallback_analysis"
        }
