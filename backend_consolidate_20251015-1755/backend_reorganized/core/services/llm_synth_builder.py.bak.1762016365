from __future__ import annotations
from typing import Dict, Any, List, Optional
from pathlib import Path
import os, re, time

from core.settings import WORKDIR_ROOT
from pathlib import Path as _P
from core.llm.async_utils import resolve_text


DEFAULT_TEMPLATES = {
    "backend/app/main.py": """from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from .routers import health

app = FastAPI(title="Forge App", version="0.1.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"],
)
app.include_router(health.router, prefix="/api")

@app.get("/")
async def root():
    return {"status": "ok", "service": "forge", "version": "0.1.0"}
""",

    "backend/app/routers/health.py": """from fastapi import APIRouter

router = APIRouter()

@router.get("/health")
async def health():
    return {"status": "healthy"}
""",

    "backend/app/requirements.txt": """fastapi==0.110.0
uvicorn==0.29.0
""",

    "README.md": """# Forge Project
Generated by Forge Synth Builder.
Run with:
  pip install -r backend/app/requirements.txt
  python -m uvicorn backend.app.main:app --host 0.0.0.0 --port 8080
""",

    "docker-compose.yml": """version: "3.8"
services:
  api:
    image: python:3.12-slim
    working_dir: /app
    command: bash -lc "pip install -r backend/app/requirements.txt && python -m uvicorn backend.app.main:app --host 0.0.0.0 --port 8080"
    volumes:
      - ./:/app
    ports:
      - "8080:8080"
""",

    ".env.example": """# Example environment
ENV=development
"""
}

def _extract_fenced_code(text: str) -> str:
    """Extrae el primer bloque ```...```; si no hay, devuelve el texto tal cual."""
    if not text:
        return ""
    m = re.search(r"```[a-zA-Z0-9_+-]*\s*(.*?)\s*```", text, re.DOTALL)
    return m.group(1) if m else text

def _ensure_str(x: Any) -> str:
    if x is None:
        return ''
    
    if isinstance(x, bytes):
        return x.decode("utf-8", "ignore")
    return str(x)

def _llm_client():
    """
    Intenta usar un cliente DeepSeek robusto. Si no disponible o modo offline,
    devuelve None para activar el generador offline.
    """
    if os.getenv("DEEPSEEK_OFFLINE") == "1":
        return None
    try:
        # Prefer robust client si existe
        from core.agents.real_intelligent_builder_agent import RobustDeepSeekClient as Client
        return Client()
    except Exception:
        try:
            from core.agents.real_project_builder_fixed import FixedDeepSeekClient as Client
            return Client()
        except Exception:
            return None

def _retry(n: int = 3, delay: float = 1.0):
    def deco(fn):
        def wrap(*a, **k):
            last = None
            for i in range(n):
                try:
                    return fn(*a, **k)
                except Exception as e:
                    last = e
                    time.sleep(delay * (2 ** i))
            raise last
        return wrap
    return deco

def _merge_structure(base, extra):
    # une por path, evitando duplicados
    seen = {x.get('path') for x in base if isinstance(x, dict)}
    for it in extra:
        p = it.get('path') if isinstance(it, dict) else None
        if p and p not in seen:
            base.append({'path': p})
            seen.add(p)
    return base

class LLMSynthBuilder:
    """
    Builder sintético impulsado por LLM:
    - Si plan.structure existe: lo usa como lista de archivos objetivo.
    - Si no, pide al LLM que proponga una estructura (JSON simple).
    - Por cada archivo, pide contenido completo y lo escribe.
    - Devuelve siempre 'workdir'.
    """

    def __init__(self, max_file_bytes: int = 2_000_000):
        self.max_file_bytes = max_file_bytes
        self.client = _llm_client()

    def _offline_suggest_structure(self, project_name: str) -> List[Dict[str, Any]]:
        return [
            {"path": "backend/app/main.py"},
            {"path": "backend/app/routers/health.py"},
            {"path": "backend/app/requirements.txt"},
            {"path": "README.md"},
            {"path": "docker-compose.yml"},
            {"path": ".env.example"},
        ]

    def _offline_generate_content(self, path: str, context: Dict[str, Any]) -> str:
        # Contenido “real mínimo” para que el proyecto arranque (FastAPI + compose)
        pn = context.get("project_name", "Forge Project")
        if path == "backend/app/main.py":
            return (
                "from fastapi import FastAPI\n"
                "from .routers import health\n\n"
                f"app = FastAPI(title=\"{pn}\", version=\"0.1.0\")\n"
                "app.include_router(health.router, prefix=\"/api\")\n\n"
                "@app.get(\"/\")\n"
                "def root():\n"
                "    return {\"service\": \"llm_synth\", \"status\": \"ok\"}\n"
            )
        if path == "backend/app/routers/health.py":
            return (
                "from fastapi import APIRouter\n\n"
                "router = APIRouter()\n\n"
                "@router.get(\"/health\")\n"
                "def health():\n"
                "    return {\"status\": \"healthy\"}\n"
            )
        if path == "backend/app/requirements.txt":
            return "fastapi==0.110.1\nuvicorn==0.29.0\n"
        if path == "docker-compose.yml":
            return (
                "version: \"3.9\"\n"
                "services:\n"
                "  api:\n"
                "    image: python:3.11-slim\n"
                "    working_dir: /app\n"
                "    volumes:\n"
                "      - ./backend:/app/backend\n"
                "    command: [\"python\",\"-m\",\"uvicorn\",\"backend.app.main:app\",\"--host\",\"0.0.0.0\",\"--port\",\"8000\"]\n"
                "    ports:\n"
                "      - \"8000:8000\"\n"
            )
        if path == ".env.example":
            return "ENV=dev\n"
        if path == "README.md":
            return f"# {pn}\n\nGenerated by LLMSynthBuilder (offline mode).\n"
        return f"// TODO generated by offline synth for {path}\n"

    @_retry(n=3, delay=1.0)
    def _llm_json(self, prompt: str) -> Dict[str, Any]:
        # Espera que el cliente exponga un método que devuelva texto JSON (o code block)
        if self.client is None:
            raise RuntimeError("LLM client unavailable")
        txt = resolve_text(self.client.generate_json(prompt))  # debe existir; si no, .generate_code o .generate_text
        from core.llm.json_extractor import extract_json
        return extract_json(_ensure_str(txt))

    @_retry(n=3, delay=1.0)
    def _llm_text(self, prompt: str) -> str:
        if self.client is None:
            raise RuntimeError("LLM client unavailable")
        # intentos: generate_code -> generate_text
        if hasattr(self.client, "generate_code"):
            out = resolve_text(self.client.generate_code(prompt))
        else:
            out = resolve_text(self.client.generate_text(prompt))
        return _extract_fenced_code(_ensure_str(out))

    def _ensure_size(self, content: str) -> str:
        b = content.encode("utf-8", "ignore")
        if len(b) > self.max_file_bytes:
            return b[: self.max_file_bytes].decode("utf-8", "ignore") + "\n# [TRUNCATED BY GUARD]\n"
        return content

    def _ask_structure(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        if self.client is None:
            return self._offline_suggest_structure(context.get("project_name", "Forge Project"))
        prompt = (
            "You are an expert project synthesizer. Propose a concise file structure "
            "for the project as JSON: {\"files\":[{\"path\":\"...\"}, ...]}. "
            "Avoid explanations. Project context:\n" + _ensure_str(context)
        )
        js = self._llm_json(prompt)
        files = js.get("files") or []
        # normaliza
        out: List[Dict[str, Any]] = []
        for f in files:
            p = f.get("path")
            if p and isinstance(p, str):
                out.append({"path": p})
        return out or self._offline_suggest_structure(context.get("project_name","Forge Project"))

    def _safe_write_file(root: Path, rel_path: str, content) -> None:
    content = _ensure_str(content)
    target = root / rel_path
    target.parent.mkdir(parents=True, exist_ok=True)
    target.write_text(content, encoding="utf-8")
def _write_with_fallback(project_path: Path, rel_path: str, content: str) -> None:
    text = _ensure_str(content or "").strip()
    if not text:
        # usar plantilla por defecto si está disponible
        default = DEFAULT_TEMPLATES.get(rel_path)
        if default:
            _safe_write_file(project_path, rel_path, default)
            return
        # si no hay plantilla conocida, escribir comentario para no dejar vacío
        _safe_write_file(project_path, rel_path, f"# TODO: content for {rel_path}\n")
        return
    _safe_write_file(project_path, rel_path, text)

def build(plan: dict, output_dir: str) -> dict:
    """
    Builds a project structure using DEFAULT_TEMPLATES
    if the plan lacks an explicit structure.
    """
    plan_structure = plan.get("structure") or []
    if not plan_structure:
        # Fallback: use all default templates
        plan["structure"] = [{"path": p} for p in DEFAULT_TEMPLATES.keys()]

    # Write each template file to output_dir
    from pathlib import Path
    outdir = Path(output_dir)
    for path, content in DEFAULT_TEMPLATES.items():
        file_path = outdir / path
        file_path.parent.mkdir(parents=True, exist_ok=True)
        file_path.write_text(content, encoding="utf-8")
    return plan
