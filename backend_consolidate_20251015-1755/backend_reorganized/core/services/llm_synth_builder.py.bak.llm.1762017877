from __future__ import annotations
from typing import Dict, Any, List, Optional
from pathlib import Path
import os, re, time

from core.settings import WORKDIR_ROOT
from pathlib import Path as _P
from core.llm.async_utils import resolve_text


DEFAULT_TEMPLATES = {
    "backend/app/main.py": """from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from .routers import health

app = FastAPI(title="Forge App", version="0.1.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"],
)
app.include_router(health.router, prefix="/api")

@app.get("/")
async def root():
    return {"status": "ok", "service": "forge", "version": "0.1.0"}
""",

    "backend/app/routers/health.py": """from fastapi import APIRouter

router = APIRouter()

@router.get("/health")
async def health():
    return {"status": "healthy"}
""",

    "backend/app/requirements.txt": """fastapi==0.110.0
uvicorn==0.29.0
""",

    "README.md": """# Forge Project
Generated by Forge Synth Builder.
Run with:
  pip install -r backend/app/requirements.txt
  python -m uvicorn backend.app.main:app --host 0.0.0.0 --port 8080
""",

    "docker-compose.yml": """version: "3.8"
services:
  api:
    image: python:3.12-slim
    working_dir: /app
    command: bash -lc "pip install -r backend/app/requirements.txt && python -m uvicorn backend.app.main:app --host 0.0.0.0 --port 8080"
    volumes:
      - ./:/app
    ports:
      - "8080:8080"
""",

    ".env.example": """# Example environment
ENV=development
"""
}

def _extract_fenced_code(text: str) -> str:
    """Extrae el primer bloque ```...```; si no hay, devuelve el texto tal cual."""
    if not text:
        return ""
    m = re.search(r"```[a-zA-Z0-9_+-]*\s*(.*?)\s*```", text, re.DOTALL)
    return m.group(1) if m else text

def _ensure_str(x: Any) -> str:
    if x is None:
        return ''
    
    if isinstance(x, bytes):
        return x.decode("utf-8", "ignore")
    return str(x)

def _llm_client():
    """
    Intenta usar un cliente DeepSeek robusto. Si no disponible o modo offline,
    devuelve None para activar el generador offline.
    """
    if os.getenv("DEEPSEEK_OFFLINE") == "1":
        return None
    try:
        # Prefer robust client si existe
        from core.agents.real_intelligent_builder_agent import RobustDeepSeekClient as Client
        return Client()
    except Exception:
        try:
            from core.agents.real_project_builder_fixed import FixedDeepSeekClient as Client
            return Client()
        except Exception:
            return None

def _retry(n: int = 3, delay: float = 1.0):
    def deco(fn):
        def wrap(*a, **k):
            last = None
            for i in range(n):
                try:
                    return fn(*a, **k)
                except Exception as e:
                    last = e
                    time.sleep(delay * (2 ** i))
            raise last
        return wrap
    return deco

def _merge_structure(base, extra):
    # une por path, evitando duplicados
    seen = {x.get('path') for x in base if isinstance(x, dict)}
    for it in extra:
        p = it.get('path') if isinstance(it, dict) else None
        if p and p not in seen:
            base.append({'path': p})
            seen.add(p)
    return base

class LLMSynthBuilder:
    """
    Builder sintético impulsado por LLM:
    - Si plan.structure existe: lo usa como lista de archivos objetivo.
    - Si no, pide al LLM que proponga una estructura (JSON simple).
    - Por cada archivo, pide contenido completo y lo escribe.
    - Devuelve siempre 'workdir'.
    """

    def __init__(self, max_file_bytes: int = 2_000_000):
        self.max_file_bytes = max_file_bytes
        self.client = _llm_client()

    def _offline_suggest_structure(self, project_name: str) -> List[Dict[str, Any]]:
        return [
            {"path": "backend/app/main.py"},
            {"path": "backend/app/routers/health.py"},
            {"path": "backend/app/requirements.txt"},
            {"path": "README.md"},
            {"path": "docker-compose.yml"},
            {"path": ".env.example"},
        ]

    def _offline_generate_content(self, path: str, context: Dict[str, Any]) -> str:
        # Contenido “real mínimo” para que el proyecto arranque (FastAPI + compose)
        pn = context.get("project_name", "Forge Project")
        if path == "backend/app/main.py":
            return (
                "from fastapi import FastAPI\n"
                "from .routers import health\n\n"
                f"app = FastAPI(title=\"{pn}\", version=\"0.1.0\")\n"
                "app.include_router(health.router, prefix=\"/api\")\n\n"
                "@app.get(\"/\")\n"
                "def root():\n"
                "    return {\"service\": \"llm_synth\", \"status\": \"ok\"}\n"
            )
        if path == "backend/app/routers/health.py":
            return (
                "from fastapi import APIRouter\n\n"
                "router = APIRouter()\n\n"
                "@router.get(\"/health\")\n"
                "def health():\n"
                "    return {\"status\": \"healthy\"}\n"
            )
        if path == "backend/app/requirements.txt":
            return "fastapi==0.110.1\nuvicorn==0.29.0\n"
        if path == "docker-compose.yml":
            return (
                "version: \"3.9\"\n"
                "services:\n"
                "  api:\n"
                "    image: python:3.11-slim\n"
                "    working_dir: /app\n"
                "    volumes:\n"
                "      - ./backend:/app/backend\n"
                "    command: [\"python\",\"-m\",\"uvicorn\",\"backend.app.main:app\",\"--host\",\"0.0.0.0\",\"--port\",\"8000\"]\n"
                "    ports:\n"
                "      - \"8000:8000\"\n"
            )
        if path == ".env.example":
            return "ENV=dev\n"
        if path == "README.md":
            return f"# {pn}\n\nGenerated by LLMSynthBuilder (offline mode).\n"
        return f"// TODO generated by offline synth for {path}\n"

    @_retry(n=3, delay=1.0)
    def _llm_json(self, prompt: str) -> Dict[str, Any]:
        # Espera que el cliente exponga un método que devuelva texto JSON (o code block)
        if self.client is None:
            raise RuntimeError("LLM client unavailable")
        txt = resolve_text(self.client.generate_json(prompt))  # debe existir; si no, .generate_code o .generate_text
        from core.llm.json_extractor import extract_json
        return extract_json(_ensure_str(txt))

    @_retry(n=3, delay=1.0)
    def _llm_text(self, prompt: str) -> str:
        if self.client is None:
            raise RuntimeError("LLM client unavailable")
        # intentos: generate_code -> generate_text
        if hasattr(self.client, "generate_code"):
            out = resolve_text(self.client.generate_code(prompt))
        else:
            out = resolve_text(self.client.generate_text(prompt))
        return _extract_fenced_code(_ensure_str(out))

    def _ensure_size(self, content: str) -> str:
        b = content.encode("utf-8", "ignore")
        if len(b) > self.max_file_bytes:
            return b[: self.max_file_bytes].decode("utf-8", "ignore") + "\n# [TRUNCATED BY GUARD]\n"
        return content

    def _ask_structure(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        if self.client is None:
            return self._offline_suggest_structure(context.get("project_name", "Forge Project"))
        prompt = (
            "You are an expert project synthesizer. Propose a concise file structure "
            "for the project as JSON: {\"files\":[{\"path\":\"...\"}, ...]}. "
            "Avoid explanations. Project context:\n" + _ensure_str(context)
        )
        js = self._llm_json(prompt)
        files = js.get("files") or []
        # normaliza
        out: List[Dict[str, Any]] = []
        for f in files:
            p = f.get("path")
            if p and isinstance(p, str):
                out.append({"path": p})
        return out or self._offline_suggest_structure(context.get("project_name","Forge Project"))

def _safe_write_file(root: Path, rel_path: str, content: Any) -> None:
    content = _ensure_str(content)
    target = root / rel_path
    target.parent.mkdir(parents=True, exist_ok=True)
    target.write_text(content, encoding="utf-8")

def _write_with_fallback(project_path: Path, rel_path: str, content: str) -> None:
    text = _ensure_str(content or "").strip()
    if not text:
        # usar plantilla por defecto si está disponible
        default = DEFAULT_TEMPLATES.get(rel_path)
        if default:
            _safe_write_file(project_path, rel_path, default)
            return
        # si no hay plantilla conocida, escribir comentario para no dejar vacío
        _safe_write_file(project_path, rel_path, f"# TODO: content for {rel_path}\n")
        return
    _safe_write_file(project_path, rel_path, text)


def synth_build(project_id: str, plan: dict) -> dict:
    """
    Deterministic synth builder:
    - Si plan['structure'] falta o es corta, usa DEFAULT_TEMPLATES.
    - Escribe en WORKDIR_ROOT/<project_id>/...
    - Devuelve {"workdir": "...", "file_count": N}
    """
    from pathlib import Path as _P
    from core.settings import WORKDIR_ROOT

    workdir = _P(WORKDIR_ROOT) / project_id if isinstance(WORKDIR_ROOT, str) else WORKDIR_ROOT / project_id
    workdir.mkdir(parents=True, exist_ok=True)

    structure = plan.get("structure") or []
    if not isinstance(structure, list) or len(structure) < 3:
        structure = [{"path": k} for k in DEFAULT_TEMPLATES.keys()]
        plan["structure"] = structure

    file_count = 0
    for item in structure:
        path = (item.get("path") if isinstance(item, dict) else str(item)).strip()
        if not path:
            continue
        target = workdir / path
        target.parent.mkdir(parents=True, exist_ok=True)

        content = DEFAULT_TEMPLATES.get(path)
        if content is None:
            if path.endswith("README.md"):
                content = "# Project generated by LLMSynthBuilder\n"
            elif path.endswith(".py"):
                content = "print('hello from forge synthesizer')\n"
            elif path.endswith("requirements.txt"):
                content = "fastapi\nuvicorn\n"
            else:
                content = ""

        # Coerción defensiva (por si llegara un awaitable)
        try:
            from core.llm.async_utils import resolve_text
            content = resolve_text(content)
        except Exception:
            pass
        if not isinstance(content, str):
            content = str(content)

        target.write_text(content, encoding="utf-8")
        file_count += 1

    return {"workdir": str(workdir), "file_count": file_count}


class LLMSynthBuilder:
    """Wrapper requerido por builder_facade: .build(project_id, plan)"""
    def build(self, project_id: str, plan: dict) -> dict:
        return synth_build(project_id, plan)

